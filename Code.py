# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.


# Import required Libraries
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

"""# Load the Dataset"""

df=pd.read_csv('/content/drive/MyDrive/B.Tech Project/oqmd_data.csv')

df.head()

df.shape

df = df.round(decimals = 2)
df.head()

"""# Exploration of Data"""

df.describe()

df.keys()

df.info()

df.isnull().sum()

"""This shows that there is no missing value in my data, so no need to do any Missing data imputation."""

# Univariate Analysis of cs column
df['cs'].value_counts()

df['cs'].value_counts().plot(kind='bar')
plt.xlabel('Crystal Structure')
plt.ylabel('Counts')

# Show the plot
plt.show()

df['cs'].value_counts().plot(kind='pie')
plt.ylabel('Counts')
# Show the plot
plt.show()

"""# Define numerical and categorical columns"""

numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']
categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']

# print columns
print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))
print('\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))

df= df.drop(columns=["name","entry_id","icsd_id","sg","cs"], errors='ignore')
df.sample()

feature_names=df.keys()
feature_names

"""# Outlier Removal"""

from scipy import stats

#  Remove Outliers using IQR method
def remove_outliers(df):
    df_no_outliers = df.copy()
    for column in df.select_dtypes(include=['float64', 'int64']).columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Remove outliers
        df_no_outliers = df_no_outliers[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

    return df_no_outliers

# Apply outlier removal
df_cleaned = remove_outliers(df)

for col in feature_names:
    plt.figure(figsize=(16,4))

    plt.subplot(141)
    sns.distplot(df[col], label='skew: '+ str(np.round(df[col].skew(),2)))
    plt.title('Before')
    plt.legend()

    plt.subplot(142)
    sns.distplot(df_cleaned[col], label ='skew '+ str(np.round(df_cleaned[col].skew(),2)))
    plt.title('After')
    plt.legend()

    plt.subplot(143)
    sns.boxplot(df[col])
    plt.title('Before')

    plt.subplot(144)
    sns.boxplot(df_cleaned[col])
    plt.title('After')
    plt.tight_layout()
    plt.show()

df_Ef = df_cleaned.copy()
df_Eg = df_cleaned.copy()

"""# Feature Selection for target as Formation Energy(Ef)

# Constant and Quasi-Constant Feature Selection
"""

from sklearn.feature_selection import VarianceThreshold

X1 = df_Ef.drop(columns=['Ef'])  # Features
y = df_Ef['Ef']

X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)

# Constant and Quasi-Constant Feature Selection
var_thres = VarianceThreshold(threshold=0)
var_thres.fit(X_train)

# Get a boolean array indicating which features are kept
support = var_thres.get_support()

# Get a list of constant columns
constant_columns = [column for column in X_train.columns if column not in X_train.columns[support]]

# Print the number of constant columns
print("Number of constant columns:", len(constant_columns))

# Print the constant columns as a list
print("Constant columns:", constant_columns)

"""# Find Corealted Pairs"""

corr_matrix = X_train.corr()
columns = corr_matrix.columns

# Create a list to keep track of correlated feature pairs
correlated_pairs = []

# Loop over the columns
for i in range(len(columns)):
    for j in range(i + 1, len(columns)):
        # Access the cell of the DataFrame and check for high correlation
        if abs(corr_matrix.loc[columns[i], columns[j]]) > 0.95:
            correlated_pairs.append((columns[i], columns[j]))

# Print the number of correlated pairs and the feature pairs themselves
print("Number of correlated pairs:", len(correlated_pairs))
print("Correlated pairs:", correlated_pairs)

"""# Drop Correlated Pairs"""

columns_to_drop = []

# Loop over the columns to identify correlated columns for removal
for i in range(len(columns)):
    for j in range(i + 1, len(columns)):
        if abs(corr_matrix.loc[columns[i], columns[j]]) > 0.95:
            columns_to_drop.append(columns[j])

# Print the number of columns to drop and the columns themselves
print("Number of columns to drop:", len(columns_to_drop))
print("Columns to drop:", columns_to_drop)

# Remove correlated columns from training data
X_train_filtered = X_train.drop(columns=columns_to_drop)
X_test_filtered = X_test.drop(columns=columns_to_drop)

"""# ANOVA Test"""

from sklearn.feature_selection import f_regression
from sklearn.feature_selection import SelectKBest

# SelectKBest to perform ANOVA Test (f_regression)
# Select the top 5 features
sel = SelectKBest(f_regression, k=5).fit(X_train_filtered, y_train)

# Display selected feature names
selected_features = X_train_filtered.columns[sel.get_support()]

# Print the selected features
print("Selected features:", selected_features)

# Subset the training and test dataframes with the selected features
X_train_selected = X_train_filtered[selected_features]
X_test_selected = X_test_filtered[selected_features]

# Display the first few rows of the new training dataframe
X_train_selected.head()

"""# Pearson Correlation"""

# Calculate the Pearson correlation matrix for the selected features in the training set
pearson_corr_matrix = X_train_selected.corr()

# Round the Pearson correlation coefficients to 2 decimal places
pearson_corr_matrix_rounded = np.round(pearson_corr_matrix, 2)

# Print the rounded Pearson correlation matrix
print("Pearson Correlation Matrix (rounded to 2 decimal places):")
print(pearson_corr_matrix_rounded)

"""# Model Training

Linear Regression
"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 1. Model training
model = LinearRegression()
model.fit(X_train_selected, y_train)

# 2. Predictions on train and test set
y_train_pred = model.predict(X_train_selected)
y_test_pred = model.predict(X_test_selected)

# 3. Regression Metrics for train and test sets
def print_metrics(y_true, y_pred, set_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    # Print the metrics for the given set (train or test)
    print(f'{set_name} Metrics:')
    print(f'Mean Squared Error (MSE): {mse}')
    print(f'Root Mean Squared Error (RMSE): {rmse}')
    print(f'Mean Absolute Error (MAE): {mae}')
    print(f'R^2 Score: {r2}')
    print('-' * 40)

# Call the function to print metrics for both train and test sets
print_metrics(y_train, y_train_pred, 'Train')  # Train metrics
print_metrics(y_test, y_test_pred, 'Test')    # Test metrics

#4. Cross-Validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
print(f'Cross-Validation R^2 scores: {cv_scores}')
print(f'Mean CV R^2 score: {np.mean(cv_scores)}')

# 5. Accuracy score on test data
test_accuracy = r2_score(y_test, y_test_pred)
print(f'Test R^2 Accuracy: {test_accuracy}')

"""# Using Optuna"""

! pip install optuna

!pip install cmaes

"""# Optimizing multiple models"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Lasso
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, mean_absolute_error, r2_score
import numpy as np
import optuna
from optuna.samplers import CmaEsSampler

# Define the objective function for Optuna
def objective(trial):
    # Choose the algorithm to tune
    regressor_name = trial.suggest_categorical('regressor', ['RandomForest', 'Lasso', 'XGBoost'])

    # Set up parameters based on the chosen regressor
    if regressor_name == 'RandomForest':
        # Hyperparameters for RandomForest
        n_estimators = trial.suggest_int('rf_n_estimators', 50, 300)
        max_depth = trial.suggest_int('rf_max_depth', 2, 20)
        min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 10)
        min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 1, 4)
        regressor = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf
        )

    elif regressor_name == 'Lasso':
        # Hyperparameters for Lasso Regression
        alpha = trial.suggest_float('lasso_alpha', 1e-3, 1e1, log=True)
        regressor = Lasso(alpha=alpha)

    elif regressor_name == 'XGBoost':
        # Hyperparameters for XGBoost
        n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300)
        learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True)
        max_depth = trial.suggest_int('xgb_max_depth', 2, 20)
        regressor = XGBRegressor(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            objective='reg:squarederror'  # Specify the objective function
        )

    # Define the scoring metric (MAE) and accuracy (R²)
    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)
    r2_scorer = make_scorer(r2_score)

    # Perform cross-validation for MAE (using 5-fold CV as an example)
    mae_scores = cross_val_score(regressor, X_train_selected, y_train, scoring=mae_scorer, cv=5)
    r2_scores = cross_val_score(regressor, X_train_selected, y_train, scoring=r2_scorer, cv=5)

    # Return the mean negative MAE as the objective value (convert to positive MAE by negating)
    mean_mae = -np.mean(mae_scores)

    # Store the R² score for later reference
    trial.set_user_attr("r2_score", np.mean(r2_scores))

    return mean_mae

# Create the study with CMA-ES sampler
study = optuna.create_study(sampler=CmaEsSampler(), direction='minimize')  # We want to minimize MAE

# Optimize the study
study.optimize(objective, n_trials=100)

# Retrieve the best trial
best_trial = study.best_trial

# Print the best trial results
print(f"Best trial MAE: {best_trial.value}")
print(f"Best R² score: {best_trial.user_attrs['r2_score']}")
print("Best hyperparameters:")
for key, value in best_trial.params.items():
    print(f"{key}: {value}")

